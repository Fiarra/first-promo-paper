{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÑ Extracting Text from Scientific Papers with Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure Python can find your scripts folder\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Import the functions\n",
    "from scripts.pdf_reader import extract_full_text, extract_partial_text\n",
    "from scripts.rename_pdfs import rename_pdfs_in_folder\n",
    "from scripts.llm_extractor import extract_title_abstract_with_llm\n",
    "from scripts.config_loader import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è Load Configuration and Define Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing Configuration:\n",
      "LLM Model: gpt-3.5-turbo\n",
      "Page Limit for Metadata: 2\n",
      "Extract Full Text: True\n",
      "\n",
      "üìÇ PDF Folder: ../data/batches/first_batch\n",
      "üìÑ Output File: ../outputs/first_batch.csv\n"
     ]
    }
   ],
   "source": [
    "# Load configuration with a specific profile\n",
    "# Options: 'high_quality' or 'fast_processing' or None for default\n",
    "# ‚¨áÔ∏è Decide which profile to use\n",
    "config = load_config('fast_processing')  # Change this to use different profiles\n",
    "\n",
    "# Print out some key configuration settings\n",
    "print(\"üîç Processing Configuration:\")\n",
    "print(f\"LLM Model: {config.get('llm.model')}\")\n",
    "print(f\"Page Limit for Metadata: {config.get('pdf.extraction.page_limit_for_metadata')}\")\n",
    "print(f\"Extract Full Text: {config.get('pdf.extraction.full_text')}\\n\")\n",
    "\n",
    "# Define batch\n",
    "# ‚¨áÔ∏è Decide which batch to use\n",
    "batch_name = \"first_batch\"\n",
    "\n",
    "# Define folder paths using the configuration \n",
    "pdf_folder = os.path.join(config.get(\"paths.data_dir\"), batch_name)\n",
    "output_file = os.path.join(config.get(\"paths.output_dir\"), f\"{batch_name}.csv\")\n",
    "\n",
    "print(f\"üìÇ PDF Folder: {pdf_folder}\")\n",
    "print(f\"üìÑ Output File: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üñãÔ∏è Rename the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Renamed 10 files\n",
      "\n",
      "üìã Sample of original filenames:\n",
      "  paper_001.pdf ‚Üê paper_001.pdf\n",
      "  paper_002.pdf ‚Üê paper_002.pdf\n",
      "  paper_003.pdf ‚Üê paper_003.pdf\n",
      "  ... and 7 more\n"
     ]
    }
   ],
   "source": [
    "# Rename PDFs using the configuration and get original filename mapping\n",
    "renamed_files, original_filenames_map = rename_pdfs_in_folder(pdf_folder)\n",
    "print(f\"üîÑ Renamed {len(renamed_files)} files\")\n",
    "\n",
    "# Show sample of original to new filename mapping\n",
    "print(\"\\nüìã Sample of original filenames:\")\n",
    "for i, (new_name, original_name) in enumerate(list(original_filenames_map.items())[:3]):\n",
    "    print(f\"  {new_name} ‚Üê {original_name}\")\n",
    "if len(original_filenames_map) > 3:\n",
    "    print(f\"  ... and {len(original_filenames_map) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Extraction on a Single PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Testing extraction on: paper_001.pdf\n",
      "\n",
      "Title: Artificial intelligence in systematic literature reviews: reduction depends on the effort required to set up the algorithm and/or query.\n",
      "\n",
      "Abstract: We read with interest the article by Qin et al, 2021, who used machine-learning (ML)-based natural language processing (NLP) for title and abstract screening. The potential of artificial intelligence (AI) for rapid screening in systematic literature reviews (SLRs) has been discussed for many years, ...\n",
      "\n",
      "üìä Confidence Scores:\n",
      "  Title confidence: 4/5\n",
      "  Abstract confidence: 4/5\n",
      "  Explanation: The title and abstract were clearly identifiable based on their content and structure.\n"
     ]
    }
   ],
   "source": [
    "# Test on a single PDF\n",
    "if renamed_files:\n",
    "    single_pdf_path = renamed_files[0]  # Use the first PDF\n",
    "    \n",
    "    # Get page limit from config\n",
    "    page_limit = config.get(\"pdf.extraction.page_limit_for_metadata\")\n",
    "    partial_text = extract_partial_text(single_pdf_path, page_limit)\n",
    "    \n",
    "    # Extract title, abstract, and confidence info using the configured model\n",
    "    title, abstract, confidence_info = extract_title_abstract_with_llm(partial_text)\n",
    "    \n",
    "    print(f\"üî¨ Testing extraction on: {os.path.basename(single_pdf_path)}\")\n",
    "    print(f\"\\nTitle: {title}\")\n",
    "    print(f\"\\nAbstract: {abstract[:300]}...\")\n",
    "    \n",
    "    # Display confidence information\n",
    "    print(f\"\\nüìä Confidence Scores:\")\n",
    "    print(f\"  Title confidence: {confidence_info.get('title_score')}/5\")\n",
    "    print(f\"  Abstract confidence: {confidence_info.get('abstract_score')}/5\")\n",
    "    print(f\"  Explanation: {confidence_info.get('explanation')}\")\n",
    "else:\n",
    "    print(\"‚ùå No PDFs found to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìú 2. Extract Text and Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 10: paper_001.pdf\n",
      "Processing 2 of 10: paper_002.pdf\n",
      "Processing 3 of 10: paper_003.pdf\n",
      "Processing 4 of 10: paper_004.pdf\n",
      "Processing 5 of 10: paper_005.pdf\n",
      "Processing 6 of 10: paper_006.pdf\n",
      "Processing 7 of 10: paper_007.pdf\n",
      "Processing 8 of 10: paper_008.pdf\n",
      "Processing 9 of 10: paper_009.pdf\n",
      "Processing 10 of 10: paper_010.pdf\n",
      "\n",
      "‚úÖ Processed 10 papers successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>original_filename</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_confidence</th>\n",
       "      <th>abstract_confidence</th>\n",
       "      <th>confidence_explanation</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paper_001.pdf</td>\n",
       "      <td>paper_001.pdf</td>\n",
       "      <td>Artificial intelligence in systematic literatu...</td>\n",
       "      <td>EXTRACTION_FAILED</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>The title was clearly identified from the text...</td>\n",
       "      <td>Journal of Clinical Epidemiology 138 (2021) 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paper_002.pdf</td>\n",
       "      <td>paper_002.pdf</td>\n",
       "      <td>Large language models for conducting systemati...</td>\n",
       "      <td>EXTRACTION_FAILED</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>The title is clearly identified at the beginni...</td>\n",
       "      <td>Journal Pre-proof\\nLarge language models for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paper_003.pdf</td>\n",
       "      <td>paper_003.pdf</td>\n",
       "      <td>Automation of systematic literature reviews: A...</td>\n",
       "      <td>Systematic literature review (SLR) studies aim...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>The title is clearly identified with explicit ...</td>\n",
       "      <td>InformationandSoftwareTechnology136(2021)10658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paper_004.pdf</td>\n",
       "      <td>paper_004.pdf</td>\n",
       "      <td>Automating Systematic Literature Reviews with ...</td>\n",
       "      <td>Objectives: An SLR is presented focusing on te...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>The title is clearly identified within the tex...</td>\n",
       "      <td>Preprint of: Sundaram, G. and Berleant, D., Au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paper_005.pdf</td>\n",
       "      <td>paper_005.pdf</td>\n",
       "      <td>Cutting Through the Clutter: The Potential of ...</td>\n",
       "      <td>In academic research, systematic literature re...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>The title is clearly indicated at the beginnin...</td>\n",
       "      <td>Cutting Through the Clutter: The Potential of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     document_id original_filename  \\\n",
       "0  paper_001.pdf     paper_001.pdf   \n",
       "1  paper_002.pdf     paper_002.pdf   \n",
       "2  paper_003.pdf     paper_003.pdf   \n",
       "3  paper_004.pdf     paper_004.pdf   \n",
       "4  paper_005.pdf     paper_005.pdf   \n",
       "\n",
       "                                               title  \\\n",
       "0  Artificial intelligence in systematic literatu...   \n",
       "1  Large language models for conducting systemati...   \n",
       "2  Automation of systematic literature reviews: A...   \n",
       "3  Automating Systematic Literature Reviews with ...   \n",
       "4  Cutting Through the Clutter: The Potential of ...   \n",
       "\n",
       "                                            abstract  title_confidence  \\\n",
       "0                                  EXTRACTION_FAILED                 4   \n",
       "1                                  EXTRACTION_FAILED                 4   \n",
       "2  Systematic literature review (SLR) studies aim...                 4   \n",
       "3  Objectives: An SLR is presented focusing on te...                 4   \n",
       "4  In academic research, systematic literature re...                 5   \n",
       "\n",
       "   abstract_confidence                             confidence_explanation  \\\n",
       "0                    1  The title was clearly identified from the text...   \n",
       "1                    1  The title is clearly identified at the beginni...   \n",
       "2                    3  The title is clearly identified with explicit ...   \n",
       "3                    4  The title is clearly identified within the tex...   \n",
       "4                    4  The title is clearly indicated at the beginnin...   \n",
       "\n",
       "                                            raw_text  \n",
       "0  Journal of Clinical Epidemiology 138 (2021) 24...  \n",
       "1  Journal Pre-proof\\nLarge language models for c...  \n",
       "2  InformationandSoftwareTechnology136(2021)10658...  \n",
       "3  Preprint of: Sundaram, G. and Berleant, D., Au...  \n",
       "4  Cutting Through the Clutter: The Potential of ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============= MAIN PROCESSING CELL =============\n",
    "# This is the main processing cell that extracts title and abstract with confidence scores\n",
    "\n",
    "records = []\n",
    "processed_count = 0\n",
    "total_files = len(renamed_files)\n",
    "\n",
    "# Get configuration options\n",
    "extract_full = config.get(\"pdf.extraction.full_text\", True)\n",
    "page_limit = config.get(\"pdf.extraction.page_limit_for_metadata\")\n",
    "model = config.get(\"llm.model\")\n",
    "\n",
    "for file_path in renamed_files:\n",
    "    processed_count += 1\n",
    "    filename = os.path.basename(file_path)\n",
    "    original_filename = original_filenames_map.get(filename, \"Unknown\")\n",
    "    \n",
    "    print(f\"Processing {processed_count} of {total_files}: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract text based on configuration\n",
    "        if extract_full:\n",
    "            full_text = extract_full_text(file_path)  # entire PDF text\n",
    "        else:\n",
    "            full_text = \"\"  # Skip full text extraction if disabled in config\n",
    "            \n",
    "        partial_text = extract_partial_text(file_path, page_limit)\n",
    "\n",
    "        # Extract title, abstract, and confidence scores\n",
    "        title, abstract, confidence_info = extract_title_abstract_with_llm(partial_text, model)\n",
    "\n",
    "        # Create record with dynamic fields based on config\n",
    "        record = {\n",
    "            \"document_id\": filename,\n",
    "            \"original_filename\": original_filename,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"title_confidence\": confidence_info.get(\"title_score\", 0),\n",
    "            \"abstract_confidence\": confidence_info.get(\"abstract_score\", 0),\n",
    "            \"confidence_explanation\": confidence_info.get(\"explanation\", \"\")\n",
    "        }\n",
    "        \n",
    "        # Only include raw_text if full text extraction is enabled\n",
    "        if extract_full:\n",
    "            record[\"raw_text\"] = full_text\n",
    "            \n",
    "        records.append(record)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        # Add error record\n",
    "        records.append({\n",
    "            \"document_id\": filename,\n",
    "            \"original_filename\": original_filename,\n",
    "            \"title\": \"EXTRACTION_FAILED\",\n",
    "            \"abstract\": f\"Error: {str(e)}\",\n",
    "            \"title_confidence\": 0,\n",
    "            \"abstract_confidence\": 0,\n",
    "            \"confidence_explanation\": f\"Processing error: {str(e)}\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"\\n‚úÖ Processed {len(records)} papers successfully\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 3. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import the validation utilities\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_validation_flags, plot_confidence_distribution, generate_validation_report\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Generate a validation report\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä VALIDATION ANALYSIS\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repo/first-promo-paper/notebooks/../scripts/validation_utils.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Tuple, Any, Optional\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd_validation_flags\u001b[39m(df: pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Import the validation utilities\n",
    "from scripts.validation_utils import add_validation_flags, plot_confidence_distribution, generate_validation_report\n",
    "\n",
    "# Generate a validation report\n",
    "print(\"\\n\\nüìä VALIDATION ANALYSIS\\n\")\n",
    "profile_name = config.get(\"_loaded_profile\", \"default\")  # We'll need to add this to config_loader.py later\n",
    "generate_validation_report(df, f\"../outputs/validation_report_{batch_name}_{profile_name}.html\")\n",
    "\n",
    "# Plot confidence distribution\n",
    "plot_confidence_distribution(df, config_name=profile_name)\n",
    "\n",
    "# Display papers with low confidence scores\n",
    "low_confidence = df[(df['title_confidence'] <= 2) | (df['abstract_confidence'] <= 2)]\n",
    "if len(low_confidence) > 0:\n",
    "    print(\"\\nüìã Papers with low confidence scores (requiring manual review):\")\n",
    "    for _, row in low_confidence.iterrows():\n",
    "        print(f\"  - {row['document_id']}: Title confidence: {row['title_confidence']}, Abstract confidence: {row['abstract_confidence']}\")\n",
    "        print(f\"    Explanation: {row['confidence_explanation']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n‚úÖ No papers with low confidence scores!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 4. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved CSV to: ../outputs/third_batch.csv\n"
     ]
    }
   ],
   "source": [
    "# Get output format from config\n",
    "output_format = config.get(\"output.format\", \"csv\")\n",
    "\n",
    "if output_format == \"csv\":\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"üíæ Saved CSV to: {output_file}\")\n",
    "elif output_format == \"json\":\n",
    "    json_file = output_file.replace(\".csv\", \".json\")\n",
    "    df.to_json(json_file, orient=\"records\", indent=2)\n",
    "    print(f\"üíæ Saved JSON to: {json_file}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Unsupported output format: {output_format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers processed: 3\n",
      "Papers with title extracted: 3\n",
      "Papers with abstract extracted: 3\n",
      "Average title length: 85.3 characters\n",
      "Average abstract length: 823.3 characters\n",
      "\n",
      "üîç Sample with original filenames:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>original_filename</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paper_001.pdf</td>\n",
       "      <td>Technische Innovation, thereotische Sackgasse?...</td>\n",
       "      <td>Technische Innovation, theoretische Sackgasse?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paper_002.pdf</td>\n",
       "      <td>The Dynamics of Political Incivility on Twitte...</td>\n",
       "      <td>The Dynamics of Political Incivility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paper_003.pdf</td>\n",
       "      <td>text-as-data-the-promise-and-pitfalls-of-autom...</td>\n",
       "      <td>Text as Data: The Promise and Pitfalls of Auto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     document_id                                  original_filename  \\\n",
       "0  paper_001.pdf  Technische Innovation, thereotische Sackgasse?...   \n",
       "1  paper_002.pdf  The Dynamics of Political Incivility on Twitte...   \n",
       "2  paper_003.pdf  text-as-data-the-promise-and-pitfalls-of-autom...   \n",
       "\n",
       "                                               title  \n",
       "0  Technische Innovation, theoretische Sackgasse?...  \n",
       "1               The Dynamics of Political Incivility  \n",
       "2  Text as Data: The Promise and Pitfalls of Auto...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display simple statistics about the extraction\n",
    "print(f\"Total papers processed: {len(df)}\")\n",
    "\n",
    "# Use raw strings (r prefix) for regex patterns to avoid escape issues\n",
    "print(f\"Papers with title extracted: {df['title'].count() - df['title'].str.contains(r'ERROR|I dont know').sum()}\")\n",
    "print(f\"Papers with abstract extracted: {df['abstract'].count() - df['abstract'].str.contains(r'ERROR|I dont know').sum()}\")\n",
    "\n",
    "# Calculate average lengths\n",
    "avg_title_length = df['title'].str.len().mean()\n",
    "avg_abstract_length = df['abstract'].str.len().mean()\n",
    "print(f\"Average title length: {avg_title_length:.1f} characters\")\n",
    "print(f\"Average abstract length: {avg_abstract_length:.1f} characters\")\n",
    "\n",
    "# Show original filename mapping\n",
    "print(\"\\nüîç Sample with original filenames:\")\n",
    "display(df[['document_id', 'original_filename', 'title']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
